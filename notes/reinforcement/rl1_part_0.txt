Hey, everyone, how's it going? My name is Randy Cogill. I will be the instructor today.
So today we'll be covering reinforcement learning, which is going to be, I believe, a new area
for you. So, yeah, let's get started. I can give you a quick introduction.
So, yeah, my name is Randy I am currently a senior research scientist at Amazon, so I'm in Amazon's AGI division, which was formerly part of Alexa AI, working on generative AI applications in support of the Alexa voice assistant as well as some some other internal applications.
Prior to that, I was an assistant professor in systems engineering for a number of years
at University of Virginia.
I was a research scientist at IBM Research, and I had also spent some time building supply
chain optimization applications for a large retail chain here in the U.S.
I did a PhD in electrical engineering from Stanford and did my master's there as well
and did a bachelor's in electrical engineering at Manhattan College prior to that.
So backgrounds all in EE have been working sort of in a mixture of both operations research
and more recently machine learning and AI.
But this topic of reinforcement learning is one that's really interesting to me because
it definitely straddles those two areas.
So before we get going, why don't we collect a little bit of information on who's here today.
So you should all have access to the chat as well as the Q&A.
So in the chat, if you could put in your name, your current role, your current location,
and your current company, we can get a sense of who else is on the call.
So I have a son, people connecting from San Jose.
I believe usually we have a lot of Bay Area folks in these classes.
Ben? Raja? Awesome. Great. Yeah. Thanks. It looks like we have about 15 folks here. So
welcome. Welcome, everyone. So I think this is probably information that's kind of generally
shared with you the start of most of these classes, but it doesn't hurt to kind of
go through how things like interaction and QA will work in this lecture. So obviously
you're strongly encouraged to interact with me
and other instructors during these classes.
That's probably where you get the most value.
So don't be shy to speak up and get clarifications.
The way we will do that today is
we'll use the Q&A feature in Zoom.
If there are sort of natural spots to stop
or there are questions that really should be cleared up
before we proceed, because maybe there's
some understanding that needs to be clarified before we go on to other topics. I'll try to
get those periodically as we go through each section. For those questions that I don't get to,
kind of in the midst of the lecture at the end of each of the blocks, there'll be basically
four blocks in this particular lecture at each of those. We'll try to go through the
rest of the Q&A queue and make sure all those questions get answered. But the Q&A feature in
there. I'll answer them by voice. I think if there's a need to sort of have more back
and forth discussion, we can unmute folks and bring them on, but otherwise we'll just
try to manage it directly through the Q&A. And then you'll have, like you do for your
other modules, you'll have an assignment, you'll have multiple choice questions. You
know, by all means, spend time on those ahead of time. We'll have a doubt session
where we can answer some questions midweek on those,
as well as a problem solution review session later on
on Thursday, where we can go through the solutions.
So yeah, use all those resources.
I'll be at those other two sessions later in the week.
And let's definitely interact as much as possible
if areas are not clear.
OK, so I think in the past you have covered,
you've had some exposure to generative AI models
and applications.
This topic that we're covering here is really quite different,
although there is a small area of overlap.
I think you might have seen text-to-text and text-to-image
generative AI models.
I think the last time I had taught this one,
I think folks had just seen text-to-text at that point.
You had learned about the architecture of large language
models and learned a bit about how large language
models are pre-trained and fine-tuned.
And one of the areas that you might have covered there is reinforcement learning with human
feedback as a way to fine tune large language models.
So that connects a little bit to what we're going to talk about today, although we're
going to get into a lot more detail about some of the underlying theory, as well
as applications of reinforcement learning that are quite different than the way that
reinforcement learning with human feedback is used in large language models.
So maybe before we proceed, we could have a quick recap quiz on some of that content
that you saw in the past, specifically how it relates to what we will talk about today.
So when you are fine-tuning a large language model by reinforcement learning with human
feedback, there are two things that we want to think about here that connect to reinforcement
learning. Number one, there's this notion of a reward function. That's something that
you have to obtain as part of that process of implementing reinforcement learning with
human feedback. So the first question is, how do you obtain your reward function?
And then once you have that reward function, what do you use it for? So maybe give that
a minute of thought, if you have a kind of crisp answer
that you want to share with the rest of the class,
by all means, put that into the chat.
And then we can discuss and link that up
to what we'll be talking about in more detail today.
I see one comment here mentioning that loss
is the negative of reward.
And that's the idea for how you use it, right?
What you want to do is you want
to tune the model's parameters so that outputs
that you generate for given input prompts,
maximize that reward, right?
Like that's sort of the target loss
that you're trying to optimize.
And so there's another question here about the reward
might depend on the task you're using the LLM for.
And so that's true.
So the loss that you're fine tuning for in that case
can depend on sort of which stage of training you're in.
So something like cross entropy is often
used when you're tuning these models to predict
to the next token given the context.
But reward can kind of capture sort
of more nuanced preferences for certain outputs over others.
So the idea here is that typically the way
reinforcement learning with human feedback is used
is when you're fine tuning one of these large language
models, often what you do as a first pass is you say,
I'm just going to give you a whole ton of text,
and I'm going to tune that model
get good at predicting what the next word is given a past context of prior words, and that's the area
where you might use something like a cross-entropy loss. Then you might do some additional fine-tuning
to sort of capture a certain style or certain alignment in the model, like you want to say,
you know, if I want to use this model to do Q&A, for example, I'm going to give it
lots of examples of question and answer pairs, and I can do some additional fine-tuning of that
model. And then usually when you use reinforcement learning with human feedback, you're trying
to capture more subjective qualities about which, you know, given two responses, which
one might be preferable to a human reviewer. And so often what you do is you have human
reviewers review multiple potential outputs. Often, you know, typically the way they're
presented is you might have two or more outputs and they're either ranked or you're
even pairwise comparisons, like A is better than B,
B is better than A. And then you learn, essentially,
a model that rewards those preferable outputs higher
than the less preferable outputs.
And then once you have that reward function,
you use it to tune your model so it's
more likely to generate outputs that yield higher reward
than outputs that yield lower reward.
So that's sort of the specific way that reinforcement learning or techniques related to reinforcement
learning are used in LLMs.
So today we're going to talk about the whole bigger, broader area of reinforcement learning
and cover a lot more ground beyond what you might have seen before.
So just to give you an overview of the agenda for today.
So, in the first hour, what I want to do is more just kind of motivate reinforcement learning,
show you some tools that you can use, some open source tools that you can use to test
out reinforcement learning algorithms, which is something that's super useful to have
at your disposal if you're actually trying to implement these algorithms, and then
kind of go through what are the key ingredients of a typical reinforcement learning setup.
When you're setting up one of these problems,
there are a few different things.
I mentioned states, actions, and rewards.
These are things that you have to model
when you're setting up a reinforcement learning
problem.
Just go through what the key concept is underlying those,
because those are something that you're just
going to see coming up over and over again when we cover
reinforcement learning.
And I'll mention in the course of this one example
that we'll use as a running example throughout this
module, and that's the game of blackjack.
right, there's a card game called Blackjack, and we'll go through the sort of general rules
of it first, but what we'll do is we'll teach a reinforcement, an agent to learn how to play
Blackjack pretty well just through reinforcement learning, by just, you know, essentially by
trying to play the game over and over and over again and improving each time it plays.
So that's where we'll also introduce this Blackjack example, and that's part of this
environment, this tool set called the Chimnasium, which is a really useful tool set for testing out
reinforcement learning algorithms. So then in the second hour, we will spend some time talking about
something called Markov decision processes. And so Markov decision processes are a class of models
for modeling problems that involve making decisions sequentially over time. There's not
going to be any learning. We're not talking about learning algorithms at that point, but
sort of talking about the fundamental sort of modeling framework that these type of
sequential decision problems can be posed and a lot of the key elements that you need to
understand to move on to learning algorithms are covered there. And then we'll see how
Blackjack can be modeled as one of these Markov decision processes and you can get an optimal
strategy out by solving that Markov decision process. It's kind of a painful process
because there's a lot of details of the model that need to be sort of painstakingly be spelled out.
But that will help motivate the next section, which is getting into learning algorithms.
We're learning algorithms. You don't have to do that explicit modeling. So we'll talk about an
algorithm called Q-learning, and then we'll see how that applies to blackjack. And then we'll
finally cover something called deep Q-learning, which is an extension of Q-learning that
that generally can be used in problems where sort of what's called the state space, you
know, certain elements of the model are too large to capture directly through tabular
Q learning. This gives us a way to solve larger problems. And this is the thing,
you know, you might have seen these, you know, deep mind examples from 10 years ago
where they taught learning algorithms how to play video games really well. Like you
things playing Super Mario Brothers and Atari 2600 games really skillfully. So deep Q learning was
one of the algorithms that was introduced to achieve that. So that's where we'll stop
at the end of this reinforcement learning part one module. Okay, so the key learning objectives
for this module are one, to understand what reinforcement learning is at a high level and
understand how it differs from other machine learning
paradigms.
So you've seen supervised learning so far.
You've seen unsupervised learning.
And this is really sort of a completely different third
machine learning paradigm that deals
with its own class of problems that's
sort of unrelated to those two.
So understand where this fits in in relation to those
things that you've seen in the past.
Understand what Markov decision processes are
and how they provide the modeling framework
that underlies reinforcement learning methods.
So there are a whole bunch of different
reinforcement learning algorithms
and in the reinforcement learning two module,
you're gonna learn a whole other class
of these policy-based methods.
But the key thing that underlies both of those
is this idea that really what you're doing
is you're using a learning algorithm
to solve this thing called a Markov decision process.
So understanding what that common modeling framework is
is super important.
And then we're going to focus for the rest of the module on these value-based methods
for reinforcement learning, specifically algorithms in the Q-learning family.
So we're going to understand what Q-learning is, the underlying concepts, and how it's
implemented in code.
So we'll look at code notebooks for each of those four sections that I mentioned
previously, and we'll see exactly how Q-learning is implemented, and you'll see a relatively
compact implementation of it and see how it gets applied to the blackjack example.
And then finally, we'll understand what deep Q learning is, understand how it's conceptually
a little bit different from the traditional tabular Q learning that we cover in the
previous section, and you'll see its implementation in code as well.
So you'll see a full reusable implementation of deep Q learning.
And I should say at this point, the expectation for the assignment that you'll be given
after this module is to implement these algorithms
on a different problem than the blackjack problem
that we cover here.
But it turns out the implementations
that you'll see for blackjack are fairly general
and they're fairly reusable.
So you might have to solve a couple of small issues
related to what's unique about this new problem.
But you'll see how what we cover here
transfers to a totally different problem.
But the same algorithms can be used.
Okay, so we'll start with the reinforcement learning intro section, and then we'll take
a short break once that's over, and then we'll move on to the remaining three.
All right, so before we get started on reinforcement learning, I want to give you a little problem
to think about.
You don't have to write out the answer right now, right?
This is something that I want you to be thinking about all throughout this module.
And the reason why I'm doing this is that it's kind of an easy problem to state.
It's just a little game, and you can
think about what strategy you would use to play this game.
But the kinds of considerations you
make when thinking about what strategy used to play this game
are exactly the kinds of things that
get modeled when we talk about Markov decision
processes.
So if you start to formulate what
the issues are in the context of this simple concrete
example, that will give you something
to base some of this more theoretical stuff
that we'll see later on against.
Let's think about this game. The idea is it's a simple betting game. I'm going to give you
a six-sided die, and you're going to roll it. If it lands on one, the game is over.
You don't win anything. That's the bad outcome. If you roll a one out of the six possible
outcomes, game's over. You don't win anything. Otherwise, if you roll two through six,
you have a choice.
You could stop right then, and if you stop right then,
I'll pay you N dollars, where N is the number
that's showing on the top of the die.
That's the number that you rolled.
Or you get to try again.
So you can already imagine if you're thinking
about strategies for playing this game.
And you could try this again.
You could try as many times as you like
until you either decide to stop
or you roll a one and the game ends.
So you can think about, you know,
just the kind of strategies you might consider.
Like, if you roll a six, you're never going to do better than that, right?
That's probably a good time to stop and collect your payout of $6 because you're never going
to see a better outcome. But what if you roll a two, right? Two's not great. You could probably
do better if you roll again. But there's a chance that you might roll a one if you roll
again, in which case you don't get $2. You get nothing, right? And so there are a number
of things going on here, right? There's this idea that you're making decisions over time,
and you might need to make those decisions multiple times
about whether you stop or keep playing.
There's uncertainty about the future.
Like you don't know what you're gonna roll in the future,
but you know whether you maybe have a better chance
of doing better than what you have right now
or doing worse than what you have right now.
And you're ultimately trying to weigh all of that together
to figure out, okay, what decision should I make
right now, should I roll again or should I stop?
So all of that that's going on in that example
is something we're gonna kind of see generalized
quite a bit in the types of problems
that reinforcement learning is designed to solve.
OK.
So what can we actually do with reinforcement learning?
What can we solve with it?
I want to switch over to these slides
because they have this animation.
And so here we have this almost kind
of disturbing-looking video of this robot that's
learning how to walk.
And it's learning how to walk on its own
by just interacting directly with its environment.
So that's sort of the key idea with reinforcement learning
is that reinforcement learning is used,
it's a class of algorithms that help,
you'd say learning agents
that can interact with their environment.
They can learn to perform some sort of task
by just continually interacting with their environment.
So what's going on here with this quadruped robot
is, you know, actually explicitly programming something like this, you know, with a control
algorithm that tells it exactly how to coordinate all of its limbs in order to walk is a complicated
task. And so the idea is that this is just going to learn to walk and not only walk,
but do it robustly in the face of possible disturbances by just interacting directly with
environment, right? So maybe it has a sense that being upright as opposed to on its back is good,
right? So there's some reward associated with being oriented upright versus on your back.
And maybe it has a target location that it wants to reach. Like it wants to reach one of the
corners of this map that it's on. So being closer to that corner versus further away from
that corner is also a good thing. So that's all the robot knows. And it knows maybe it
actuate certain limbs, but it has no concept of cause and effect, like how actuating its limbs will
get it to achieve its goals. But it just starts interacting with its environment. It starts
learning what's getting it closer and what's getting it further away from achieving its
goals, and it keeps doing the things that seem to work and not doing the things that don't seem
to work, and it gets better and better and better as it goes. So in this case, there's
specific algorithm that's being used here that, you know, in about 30 minutes, the robot seems
to learn to almost upright itself. It's almost getting itself on four legs. And after additional
practice with this, it starts to actually be able to walk. Then after being exposed to
disturbances, like somebody hitting it with this cardboard tube here, it starts to learn how
to recover from those kinds of disturbances and gets to the point where it actually becomes
pretty skilled at performing this task of walking
and maybe getting itself to a specific goal location.
So yeah, one thing about reinforcement learning
is it's used to control systems like physical systems
like robots, it's used in self-driving cars.
And so that's probably the biggest area
where you'll see this used.
Robotics is a huge area for reinforcement learning.
But it's also something that you see frequently
in other types of environments
where decisions need to be made under uncertainty.
So games are a really popular kind of demonstration area
for reinforcement learning.
In fact, there's serious applications there
as games as well.
One of the very, very earliest implementations
of something like Q-learning, I guess
that's probably going back almost 30 years,
was the development of a learning algorithm
learned how to play backgammon, the board game backgammon, really well. In fact, it was better
than, it got to the point where it was better than most human players but also played very
differently than human players and human players ended up learning a lot about how it played and
it actually ended up influencing the way expert players actually played the game.
So it's used in games and there are also some applications in finance as well.
And so somebody asked a question, could this be used in public policy decisions? And
In principle, yes, right?
It's something that I don't know of any application there.
I think the key thing is being able
to get rapid feedback, right?
You try a thing, you get some rapid feedback
about whether or not it worked and you try again,
and you can just try again and again and again.
I guess public policy decisions can be tricky there
because maybe you don't have that mechanism
to get rapid feedback.
But in principle, like if you could maybe
use some sort of simulations to sort of bootstrap
and get started, maybe something like this could be used.
The broader field of operations research and Markov decision
processes I know are used much broader in areas of planning,
where you can maybe explicitly model the environment
that you're operating in a bit better.
But yeah, definitely kind of hold on to that idea.
If that's an area that you work
in as we're shaping up kind of the general class
of models, I'd be curious to hear
if you're seeing a match there.
OK.
So as I said, reinforcement learning
is aimed at teaching agents to take actions in an environment
in order to maximize some cumulative reward.
It might be some reward that you only earn once,
like in our dice game.
You get paid once when you decide to stop the game.
Or it can be some reward that you're just
collecting over time and you're making decisions
continually over time.
Either one, you could imagine in our dice game, you get zero reward until the end.
You still have this notion of cumulative reward, and we'll get into a lot more detail on this later.
But the idea is that an agent is interacting with its environment, it's trying out different actions,
it's getting feedback, and it's ultimately aimed at maximizing some measure of reward.
So that's kind of nicely captured in this diagram here.
And we'll see lots of examples where we see this in action.
But here you have your learning agent
that is interacting with its environment
here by taking actions.
Once it interacts with its environment,
it collects some feedback through measurements
of the environment.
Here, we'll call this a state.
And we'll say more about what the state actually
generally means.
But you could think of this as the measurements
of your environment that you take.
And you're also getting some feedback about whether you're achieving reward or you're actually
incurring penalties, and that's going to guide you towards trying out more actions that
hopefully will earn rewards rather than penalties, and you'll over time be adapting this agent's
behavior so that after a while, you have a policy of being able to do what you want
do skillfully, where being successful is measured in terms of this reward, achieving high reward.
So this is pretty different than what you've seen before. I'm assuming that the main areas
that you've seen before are supervised learning, unsupervised learning, and most of the learning
paradigms and the types of problems that you've been exposed to fit into one of those two
paradigms. And this sequential and interactive nature of reinforcement learning is one of the
key things that differentiates it from supervised learning and unsupervised learning. To greatly
simplify the whole picture, what you're doing when you do supervised learning is you're basically
learning general rules that can accurately map input features to outputs. And you're learning
those through examples, whether it's regression or classification. You're given lots of examples
of inputs, lots of examples of what should be the outputs,
then you're learning some sort of model
that can accurately predict those outputs
from those inputs.
And unsupervised learning is doing
something kind of similar, although you don't necessarily
have labels, right?
Pure unsupervised learning, you have no labels.
If you're doing something semi-supervised,
you might have some labels, but many examples
that aren't unlabeled.
But there, you're more learning
the structure of the data from lots of unlabeled data.
Here are lots of inputs.
So OK, I'm going to learn that these things basically
break into certain clusters.
And maybe those clusters have specific meaning to them.
But in all these cases, you're
learning something about relationships
among data items from individual examples.
And here we're doing something very different.
We're directly interacting with some environment.
we're getting feedback about what happened when we did something, right? What was the
outcome of our action? And we're using that to tune some sort of policy that helps us
interact effectively with the environment. So it's very different conceptually than what
you've seen with supervised and unsupervised learning. So in terms of, you know, reinforcement
learning itself, it's a really big area. It's a really broad area in terms of the amount of
There are lots of different algorithms.
We're only going to focus on two of them in this module.
But the majority of the work,
the most commonly known algorithms,
I would say fit into these two based categories
where these are model-free.
These are model-free methods,
meaning that we don't have a model of the environment.
We don't start with a model of the environment.
We don't actually ever explicitly learn
model of the environment. There are some artifacts that we end up learning that help us come up with
policies, but we don't necessarily ever have a direct model saying, oh, if I do A, then B
might happen. And these basically break into what are called value-based methods and policy-based
methods. So the value-based methods are the ones that we're going to talk about today.
And when you get to the reinforcement learning part two module, you'll talk about what are
called policy-based methods, which is a different concept, but ultimately aimed at solving the
same problem.
And then there are also other approaches like model-based methods that try to explicitly
learn a model of the environment and things like multi-agent reinforcement learning that
try to learn policies separately for lots of interacting agents.
Those are other areas of this very broad field of reinforcement learning.
We're focusing on these value-based and policy-based methods, which I'd say 90% of the time when you talk to somebody about reinforcement learning, that's probably what they're actually talking about.
So we're going to cover the big ones in these two modules.
So if you wanted to have a broad taxonomy that kind of captures all of the key algorithms that have been developed in this field,
you know, it might look something like this, where the model-free methods are what we're
focusing on in the next two modules. Q-learning, these are the value-based methods that we
talk about today. Policy optimization, those are the policy-based methods that you'll
talk about in the reinforcement learning part two module. So you're covering a good
part of that taxonomy between these two modules.
Okay, so I want to show, okay, so we had talked about reinforcement learning and how it's
different from supervised learning and unsupervised learning.
When you want to work hands-on with supervised learning, what do you do, right?
Well, you get access to some sort of data set that you can build a model off of and
then test your model on, right?
That data set might have a lot of input data
that you're going to extract some features from and maybe
some labels that you want to try to learn.
But you get this data set.
You can get this data set.
You can train the parameters of a model
against that data set if you're
building some parametric model.
And then you can test to see how accurately it
can predict the labels.
But that's the thing that you
need when you want to work hands-on with this.
Well, what do you do when you
work hands-on with reinforcement learning?
Like, do you need a robot?
You need some sort of environment
that you can interact with and that you can actuate
and that you can measure the responses from.
And that's not necessarily gonna be like some data set,
like a data set with labels and features
that you have when you're doing supervised learning.
So how do you work hands-on
with reinforcement learning algorithms
as you're learning this stuff,
if it requires interacting
with some type of environment interactively.
And so there's one really nice toolkit
that we're gonna use to demonstrate
reinforcement learning here.
You're gonna use it in your assignments that you work on.
And in fact, it's used very heavily,
even in academic literature on reinforcement learning.
It's kind of a go-to place for testing out
new algorithms.
And that's this toolkit called Gymnasium.
So this used to be maintained by OpenAI.
It was called OpenAI Gym.
I think OpenAI has kind of shifted focus away from maintaining this because they've got
bigger and better things, I guess, that they're focusing on right now, but this is now being
maintained by another organization, and it's still open sourced, and it's still very
similar to the original format of OpenAI Gym.
So I could show you what this looks like.
So you can find this by if you just search for open AI gym or forama gymnasium so forama is the name of the organization that's now maintaining maintaining this, you'll you'll you'll be able to access this as well.
And so basically what it is is it's a set of simulation environments that can be used for testing reinforcement learning that use a common interface.
interface. So there's a simple common interface to these environments. Once you learn how to
interact with one of them, you can interact with any of them, which is nice. If you're
trying to develop an algorithm that can work broadly across a whole bunch of different
domains, this gives you a way that you can quickly test your code on lots of different
examples, whether they're video game control examples, robotics examples, it's all a
see what's going on with one of these environments.
This is this lunar lander environment
where you've got this ship that you're trying to learn.
You're trying to land between these two flags.
And you can actuate it by firing thrusters
on different sides of the ship.
And the idea is you want to land softly.
You don't want to come to a crash landing between them.
You don't want to land outside the flags.
And just hopefully what's going on in this animation
is that this learning agent is learning
how to perform this task successfully through repetition.
And even on the main page here, there's
a quick code snippet right here that
shows what that interface looks like when
you're interacting with it.
So almost any time you're setting up
one of these environments, and we'll
see this in greater detail when we go through Blackjack,
There's one line that can be used to launch a new environment, select the environment
that you want to work with, like the specific simulation environment that you want to work
with.
And then you typically just have some loop where in that loop you have whatever code
you've implemented to select the actions that you want to take to try to actuate
your environment.
And then you pass that into your environment, you see what happens.
Like, how does that, you know, what do I observe about changes to my environment after I've
applied that action?
And then did I achieve any sort of reward or pay any sort of cost by doing it?
And then you just keep looping.
And somewhere you have your learning algorithm implemented, which is hopefully coming up
with smarter and smarter ways to select these actions.
But that's essentially all that happens.
And then you can close out your environment when you're done.
So it's a super, super simple interface that's provided to all these different environments
when you're testing out your algorithms. So just to give a sense of what types of
environments you have available, there are these sort of simple classic control ones
where you want to control these relatively simple physical systems. So this cart pull
example is actually what you will have in your assignment this week. So you'll implement
reinforcement learning algorithm to solve this cart-pole example. And so what cart-poles trying
to do is it's basically you have a vertical pendulum, so that's unstable, right? It wants
to fall from one side to the other, or one side or the other, depending on whether it's leaning
one way or another. And you can control it by moving a cart that the pole rests on.
Turns out to be harder than it looks, right? It looks like it's one of these things. I know
if it starts to fall this way, I'll just move the cart a little bit that way and it
will upright it again. Turns out to be actually quite a bit harder than that.
But yeah, this whole class of examples,
these are all like physical systems,
like simple physical systems that you can learn
how to control through reinforcement learning algorithms.
There are these various robotics environments
that involve more complicated robotics type setups
than you have in those simple classical control examples.
So these are again, physical systems,
but they're ones with many more degrees of freedom.
Like you hear you have a humanoid robot
that has a number of limbs
and can be actuated at each of those limbs,
and you're trying to teach this robot
maybe to stand up on its own
from an arbitrary position laying down
and then remain standing once it's standing.
So there are a whole bunch of these
various robotics type environments here.
And you have these relatively simple environments,
simple in the sense that there are relatively
fewer degrees of freedom than you have
in these robotics examples.
Like one of them here is the game of Blackjack,
which is what we're going to be working with today.
But several others that look like very simple video game
environments.
And then maybe to look at one more,
and there are others that we won't look at here.
But another big one is they have environments
for many of these classic Atari 2600 video games.
So, yeah, these are all from a video game system
that came out in, I think, the early 1980s.
But that was one of the big developments of DeepMind
about 10 years ago, where they demonstrated
deep Q learning algorithms, which we'll cover later today,
could actually learn how to play arbitrary Atari 2600
video games pretty well, just by repeated attempts
at playing them.
So those are all part of part of gymnasium as well.
So you have a lot of stuff there to experiment with
if you're interested in reinforcement learning
and you want to get hands on with it.
So let's see what one of these environments
actually looks like in action.
So we'll work with the Blackjack environment
from Gymnasium.
And before we look at it, I know
folks might be familiar with how Blackjack works.
But for people who haven't seen Blackjack before or ever
played Blackjack R21, as it's sometimes called,
I'm just going to give a really quick overview
of the rules just so you can follow along with the example.
So the idea is to card game.
And typically the player is playing against a dealer, and the player is dealt two cards to start.
And the value of their hand is worth the sum of the values on the cards.
So I give you two cards. If you have a six and a three, your hand is worth a nine at that point.
And the idea is you want to get as close to 21 as possible without going over.
If you go over 21, you lose, you just lose automatically.
If you don't get close to 21,
there's a chance that you're gonna get beaten
by the dealer, right?
So the idea is that you start off with these two cards,
you want the value of your hand is equal
to the sum of the values of the cards,
and you wanna get as close to 21 as possible.
So you can request additional cards one at a time.
Each time you request an additional card
that will increase the value,
but it also increases the risk of going over 21.
So, you know, cards with cards that have numbers on them are worth the numerical value on the cards, jacks, queens and kings are worth 10 points, and then an ace can be worth one or 11.
So you have that choice right so if it be having it be worth 11 gets you closer to 21 that's great if having it be worth 11 gets you over 21 that he can it can be worth one.
And so the idea is the player keeps taking cards until they either go over 21 or they decide to stop, and then the dealer plays. And then when the dealer plays, they keep taking cards until they either beat the player's hand or they go over 21.
If the dealer goes over 21, dealer loses, player wins. Otherwise, if the dealer beats the player's hand, the dealer wins. So that's basically the idea of the game.
And so we'll see how this works for in Gymnasium.
So we'll go through a notebook here, and I'll go through each of the steps that will show us how to actually set up Gymnasium before we run our environment.
So you can just kind of see how you can do this on your own if you're starting from scratch.
And so the first thing that we need to do, the only dependency that we need to install here is
I'll make this a little bit bigger, is just gymnasium.
And that's just a pip install gymnasium
to get that working.
And then for this notebook,
and this is gonna be the same notebook we'll use
for all of our examples.
We will set up all the packages that we'll use here.
So we're gonna use PyTorch only at the very end
when we do deep reinforcement learning
and deep Q learning, we'll use PyTorch.
So we don't need to worry about that at first.
We'll import gymnasium.
And then we're just actually importing some packages
that will help us display outputs
when we're running gymnasiums.
And then some for random number generation.
But most of these other ones
are just ones that we'll use later
when we get to deep reinforcement learning.
So it's a pretty straightforward setup.
And now what I'll do is I'll demonstrate
how Blackjack works by...
I'll play the part of the agent, right, the decision-making agent.
We won't implement an algorithm quite yet, but we'll do that in just a minute.
And so I'll sort of walk through the situation where dealer deals me a hand.
I look at that hand.
I make a decision about what action I want to take next.
I can make that decision, and then we can see the impact it has on the environment.
So in order to set that up here, that's all happening in this one cell where the
thing we're going to do is we're going to create the Blackjack gymnasium environment. So, we'll
launch the Blackjack environment. I had mentioned before it's kind of a one-liner to get to pick
your environment and get it started. This line here will deal the cards, right? So, that will
sort of set up the initial random configuration of our environment. In the case of Blackjack,
This is, you can think of this as dealing our cards.
I'll talk a little bit more about what all these things mean
when we get deeper into just sort of the generalities
of reinforcement learning.
But you can think of this as being,
you take your first observation
of the state of your environment
when it's been randomly initialized.
Then what we'll do is we'll actually
just display a visualization in the notebook
just showing what the dealer's hand looks like
and what the player's hand looks like.
This isn't really necessary if you're running some learning
algorithm behind the scenes, but it's useful if you're
interacting, if I'm viewing this directly myself.
So I'm going to render a visualization
of what the current configuration looks like.
And then I'm just going to loop.
I'm going to loop one and just make my decisions
to play the game interactively until the game ends.
And so as long as we're playing the game,
The first thing I'm gonna do is I'm gonna be prompted
to see whether or not I wanna take a new card.
That's what we call hitting.
Do I wanna take a new card or do I wanna stop
and stay with the hand that I have right now?
And that's called standing.
So I'm gonna play the role of the decision-making agent
so I'll make that decision in each step.
Once that action has been chosen,
we'll pass it to the environment
and we'll see what happens, right?
Like, okay, I'm dealt a new card.
Does that new card put me over 21?
if it doesn't, what's my new hand value? And so on. And then I'll just update the display
of the environment. So that's all we're going to do here, right, just to sort of show how
gymnasium works. So in this case, this is the initial state that we've been dealt
where I have hand worth 15.
Part of that hand is composed of an ace.
It's saying usable ace here, meaning I have an ace.
It's probably the case that I have two cards.
It doesn't show me my two cards, right?
But I'm going to assume it's an ace and a four, right?
Because it's an ace that I'm probably using as 11 plus a four.
That's giving me a total of 15.
And then this is the dealer's hand.
And the funny thing about the dealer with Blackjack
is that they show one of their two cards as I'm playing.
So I have some kind of hint about what the dealer has
in their hand.
And then I can make my decisions to play from there.
And for some reason, I'm not seeing the, here, let me,
I should be prompted to, I should be prompted here
to take an action.
OK, yeah, there we go.
Yeah, so here's a situation where, I just
dealt a new hand here just because I don't know why it
wasn't showing before.
But here, I have two cards worth 11.
The dealer is showing an ace.
I can decide whether I want to hit or stand.
I'm going to hit.
I'm going to take another card.
So I take another card.
I must have gotten an 8, because now my hand
is worth 19.
I'm not going to take another card if I'm 19,
because that's probably going to put me over 21.
So I'm going to stop there.
I'm going to stand.
And then it's going to tell me what happened.
Okay, and the only thing, the only information I get back from this visualization, it's not
telling me exactly what the dealer's hand is, but it's actually telling me that I lost.
So somehow or another, even though I had 19, which is pretty close to 21, the dealer
must have beaten me.
The dealer must have gotten a 20 or a 21.
It's entirely possible they could have a card worth 10 sitting underneath this
one right here, and as soon as they flipped it over, they had 21 and they beat me.
And so this over here that I'm printing out is the reward.
In this case, because I lost,
I earned a reward of negative ones.
Had I won, I would have earned a reward of one, right?
So this is the reward signal that comes back.
So that's just to give you an idea
of what gymnasium looks like.
Let's, before we close this part out,
Let's take a look at what it actually looks like if we actually run an algorithm,
something that in a more automated way can play blackjack.
How would we hook up an automated strategy for playing
blackjack to this gymnasium environment and have it play interactively with this environment?
So we'll do a super simple heuristic strategy before we get into any learning,
which we'll do later on in this module. So a few notes about the code sample that we just saw.
So we created our environment using this gym.make, right? And that was the one
liner I had mentioned before that, you know, is kind of part of this common
interface to all these different gymnasium environments.
We initialize our environment by calling the reset method on the environment.
And that, in this case, deals the cards.
When we initialize the environment,
we get back a snapshot of the current state
of the environment.
This is our observation of the environment.
And that tells me the current total
of what my cards are worth, what the one card
that the dealer is showing, like in the last hand
we saw they were showing an ace, and then whether or not
I have a usable ace that can be converted from 11 to 1.
So that information is given to me at the start.
I'm going to iterate over turns, where in each turn
I'm going to choose an action.
Each time I choose an action, I pass it to the environment.
And passing it to the environment
and calling the step method on the environment
updates the environment in response to my action.
And then when the environment's updated,
I get a new state that gives me my new total of my cards.
the dealer card isn't going to change and it's going to tell me, you know, maybe whether or not I still have a usable ace or maybe I picked up a new one.
And when I call this step, it also gives me the reward, right? So you can make sure that picture that we looked at before where user takes an action, they see the updated state and they see the reward.
This is also going to tell me the reward in each step. It's going to tell me if I get a reward of one if I won the hand, negative one if I lost the hand and zero otherwise.
And so, yeah, there's a question here.
What is the final outcome?
A model which is capable of playing blackjack on its own?
Exactly.
Right.
So what we're going to see here is we're going to see I can give a generic learning
algorithm to this environment without telling it anything about the rules of blackjack.
And after playing, you know, whatever, I think I'd give it 500,000 attempts to
play, 500,000 hands to play, 500,000 hands to play, it will learn how to play blackjack
pretty skillfully.
So that's the end goal for all of this.
What we'll just to understand how this code works,
because this is what we're going to be working with here
and what you'll be working with in your environment,
we're kind of building up incrementally.
And so the next thing we'll show is
how would you plug in a given policy?
Like if I had a given heuristic policy
for playing blackjack,
how would I plug it into this environment?
And then what you're gonna do eventually is,
or what we're gonna do here eventually is
not give it a heuristic policy,
We're just going to have it learn its own policy from scratch.
So OK, so one other question.
No, so it's not a one-turn game.
It's a game where you're playing multiple turns, right?
If the dealer is close to 14 and we have 10,
I can say, I want another card.
And I can ask for another card.
And maybe if my other card is 8, I get an 8,
and my 10 now becomes an 18.
And then I'm prompted again.
Now, do you still want another card?
or do you want to keep, or do you want to stop there?
So I can take it, I can take multiple turns
in that I can keep asking for new cards.
The way it's presented here,
we don't see that iterative process for the dealer, right?
We just see that the dealer had a hand,
I decided to stop,
and then we just see the dealer's final hand
after they've pulled all of their cards.
But for us, we get to make multiple,
we can make up potentially multiple decisions.
We might make a single decision, right?
We might get our initial hand and say, that's good enough.
I want to stop there.
But we can take multiple turns, and we're getting feedback
at each of those turns.
So just to see how a simple policy might look for this,
let's imagine that we play the policy where
if my hand is worth less than 17, I take a new card.
So I hit if I'm less than 17.
And if I'm 17 or above, I stop.
So that's going to be my policy.
So I've implemented this just as a simple function,
where it's going to take this state, right?
The state's going to be this triple, right?
This player, current, total, the dealer,
upward-facing card, and whether or not I have usable A's.
It's going to take those as input.
And if I'm less than 17, I'll return one,
meaning I want to hit, right?
I want to take another card.
And if it's 17 or greater, I'll return zero,
saying I want to stop.
So it's just a simple heuristic to get us started.
So now I'm going to introduce this function
that we're going to reuse throughout this module.
You can even reuse something like this
when you do your assignment, which
allows us to run a simulation on an environment
multiple times.
So this will say, I'll tell you the number of games
I want to play.
And you can give me a function that implements your policy.
So in this case, that's this function right here.
And then it's just gonna play as many games
as I specified.
And in this case, it's gonna give me my win rate, right?
So if I say, you know, play 50,000 games,
use this policy to play them,
it's gonna tell me what fraction of games
I actually won using that policy.
And so the idea here is that
In this way, this function will work.
I'll create my environment.
I'll just keep track of the total number of wins
over all of the games that I play.
And I'm just going to iterate over the number of games
I specified.
Each time I play a new game, I reset the new environment.
So I deal the cards for the first time.
I select an action based on the policy
that was provided in my policy function.
In this case, it's less than 17, take a new card,
17 or greater, stay.
I update my environment, and then if the game ends,
I update the count of wins, right?
So if the game ended and I got a reward of one,
that means I won the game,
and so I update my count of wins.
And then when I'm done with the whole thing,
I close out the environment,
and then I'm just gonna return
the fraction of games that I won.
So we could see how this works now when we play our simple policy here for 50,000.
I could actually run that cell.
Okay, so now we can see how that works if I play our simple policy for 50,000 hands.
So I'll just take a second just to run through all of these.
But yeah, when we're done, we won 41.26% of hands.
And so, you know, because there's some randomness in the simulation, it can vary each time we run it.
But, yeah, here we expect it to win around 40 to 41 percent of times.
OK, so that shows us how to actually interact with our environment.
Now, our ultimate goal is going to be to come up with a sort of more systematic way to learn how to play Blackjack on its own without knowing the rules of the game or any hints for strategy.
Okay, so before we wrap up this part and take a break,
I wanna highlight a few key concepts
that are concepts that we're gonna see over and over again
and they're things that we can now link back up
to Blackjack to get some sort of
concrete understanding of them.
And so those key elements here are the notions
of states, actions, and rewards, right?
The states being like the observations
that we could make at each period,
Like, what the total of my hand was worth, whether I had a usable ace and what card the dealer was showing, the actions being the decisions that I get to make at each point in time, whether or not I want to take a new card or stand, and then the rewards. In this case, the reward being a signal that tells me whether I won the game or not, or whether the game is just in progress and I didn't win or lose. I just got a reward of zero.
So let's take take a few minutes to just go through each of those for this example.
And this will help us build some strong intuition for what these things are.
Okay, so the state, so you can think of the state as being.
Certainly in blackjack, but more broadly, it, it, it's something that tells you everything you need to know.
About both the past and present.
In order to model where the game might go in the future.
So, this graphic here, this is sort of illustrating the state, and this is the thing that was displayed in our notebook each time we took an action.
This tells us, again, what the total value of our hand is, whether we have a usable ace, and what card the dealer is showing.
And when I say it tells you everything you need to know about the past and the present in order to model where the game is going in the future,
The idea there is it doesn't matter how I got to 20.
It doesn't matter whether I had a seven, a three, and then a card worth a 10.
It doesn't matter whether I was dealt a king and a 10 on my first hand.
It doesn't matter whether I have four cards that are, or in this case, I have a usable ace, right?
So I know I have at least an ace.
But it doesn't matter whether I have, you know, ace three six or I was dealt ace six three.
All that matters is that's where I am right now.
And that's the only information that's relevant to what might happen in, you know, subsequent plays.
And so the idea is that all of these problems, we have something like this, where there's something that sort of summarizes everything you need to know about this system in order to kind of say where it might go next.
